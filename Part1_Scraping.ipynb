{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5ea2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb9fb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url): \n",
    "    page = urllib.request.urlopen(url)\n",
    "#Cooking the Soup\n",
    "    soup = BeautifulSoup(page,\"html.parser\")\n",
    "\n",
    "    web_list=[]\n",
    "#Scraping \"Link\" (href)\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        web_list.append(a[\"href\"])\n",
    "    speech=[]\n",
    "    for i in web_list:\n",
    "        if i.startswith(\"speeches\"):\n",
    "            speech.append(i)\n",
    "    speech_web=[]\n",
    "    for i in speech:\n",
    "        if i.endswith('htm'):\n",
    "            speech_web.append(i)\n",
    "    \n",
    "    return speech_web\n",
    "\n",
    "def modify_url(speech_web):\n",
    "    start = speech_web.index('speeches/barackobama/barackobamainauguraladdress.htm')\n",
    "    speech_president = speech_web[start:]\n",
    "\n",
    "# remove replicate\n",
    "    new = []\n",
    "    for i in speech_president:\n",
    "        if i not in new:\n",
    "            new.append(i)\n",
    "\n",
    "    speech_url = []\n",
    "    for val in range(len(new)):\n",
    "        speech_url.append(\"https://www.americanrhetoric.com/\" + str(new[val]))    \n",
    "    return speech_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ee30249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(url):\n",
    "        user = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "        resp = requests.get(url, headers=user)\n",
    "        content = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        t = str(content.title)\n",
    "        title = t[t.find(\"Obama\") + 7:t.find(\"</title>\")].strip()\n",
    "        title = re.sub(r'\\r\\n', '', title)\n",
    "        speech = content.get_text(strip=True)\n",
    "        return title, speech\n",
    "    \n",
    "def useful_speech(speech):\n",
    "        useful = speech[speech.find(\"from audio]\") + len(\"from audio]\"):speech.find(\"Book/CDs\")].strip()    \n",
    "        return useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe1b0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(url):\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page,\"html.parser\")\n",
    "    year = [\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"]\n",
    "    date_list = []\n",
    "    for i in range(len(year)):\n",
    "        date_list.extend(soup.find_all(string=re.compile(year[i])))\n",
    "    s= ['Press Conference on 2010 Budget Sent to Congress','Announcement of 2012 Presidential Candidacy',\n",
    "          'Honoring Golden State Warriors 2015 NBA Champs','Press Conference Following 2016 NATO Summit']\n",
    "    for i in range(len(s)):\n",
    "        date_list.remove(s[i])                         \n",
    "    date = date_list[date_list.index(\"20 Jan 2009\"):]\n",
    "    date = [str(x) for x in date]\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa9c74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = \"https://www.americanrhetoric.com/barackobamaspeeches.htm\"\n",
    "    speech_web=get_url(url)\n",
    "    speech_url=modify_url(speech_web)\n",
    "    speeches = [extract_text(url) for url in speech_url]\n",
    "    date=extract_date(url)\n",
    "    \n",
    "    title=[]\n",
    "    speech=[]\n",
    "    for i in range(len(speech_url)):\n",
    "        title.append(speeches[i][0])\n",
    "        speech.append(useful_speech(speeches[i][1]))\n",
    "       \n",
    "    # write df_speech into csv file        \n",
    "    df_speech = pd.DataFrame({'date': date,'title':title,'content':speech})\n",
    "    df_speech.to_csv('obamaspeech.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "978791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
